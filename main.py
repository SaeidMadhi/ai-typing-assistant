import os
from collections import defaultdict, Counter
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# ğŸŒŸ ÙÛŒÙ„ØªØ± Ø¬Ø§Ø¯ÙˆÛŒÛŒ ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ğŸŒŸ
# ==========================================
def normalize(text):
    text = text.lower()
    replacements = {
        'Ä±': 'i', 'ÅŸ': 's', 'Ã§': 'c', 'ÄŸ': 'g', 'Ã¶': 'o', 'Ã¼': 'u',
        'iÌ‡': 'i'
    }
    for tr_char, en_char in replacements.items():
        text = text.replace(tr_char, en_char)
    return text

# ==========================================
# Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯
# ==========================================
vocab_dict = {}
bigram_model = defaultdict(Counter)
concat_dict = {} # ğŸŒŸ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¨Ù‡ Ù‡Ù… Ú†Ø³Ø¨ÛŒØ¯Ù† (Ù…Ø«Ù„ iyimisin)

def train_model(file_path):
    if not os.path.exists(file_path):
        return
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            words = line.strip().split()
            if not words: continue
            
            for i in range(len(words)):
                # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©Ù„Ù…Ø§Øª ØªÚ©ÛŒ
                norm_w = normalize(words[i])
                if norm_w not in vocab_dict:
                    vocab_dict[norm_w] = words[i]
                
                # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©Ù„Ù…Ø§Øª Ø¨Ø¹Ø¯ÛŒ Ùˆ Ú©Ù„Ù…Ø§Øª Ú†Ø³Ø¨ÛŒØ¯Ù‡
                if i < len(words) - 1:
                    norm_current = normalize(words[i])
                    original_next_word = words[i + 1]
                    bigram_model[norm_current][original_next_word] += 1
                    
                    # ğŸŒŸ Ø¬Ø§Ø¯ÙˆÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„ ÙØ§ØµÙ„Ù‡: Ø°Ø®ÛŒØ±Ù‡ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ù…ØªÙˆØ§Ù„ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ú†Ø³Ø¨ÛŒØ¯Ù‡
                    norm_next = normalize(words[i+1])
                    concat_norm = norm_current + norm_next
                    
                    # Ø§Ú¯Ø± ØªÙˆ Ù…ØªÙ† Ø¨ÙˆØ¯Ù‡ "iyi misin"ØŒ ØªÙˆ Ø°Ù‡Ù† Ù¾Ø§ÛŒØªÙˆÙ† Ù…ÛŒØ´Ù‡ "iyimisin"
                    if concat_norm not in concat_dict:
                        concat_dict[concat_norm] = f"{words[i]} {words[i+1]}"

print("Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ø¶Ø¯Ù Ú†Ø³Ø¨Ù†Ø¯Ú¯ÛŒ Ú©Ù„Ù…Ø§Øª...")
train_model("fa.txt")
train_model("en.txt")
train_model("tr.txt")
print("Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯! Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø§Ø±.")

# ==========================================
# API Ø§Ø±ØªØ¨Ø§Ø·ÛŒ
# ==========================================
@app.get("/suggest")
def get_suggestions(prefix: str = "", previous_word: str = ""):
    suggestions = []
    
    if prefix:
        norm_prefix = normalize(prefix)
        
        # Û±. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú©Ù„Ù…Ø§Øª ØªÚ©ÛŒ (Ù…Ø«Ù„ merhaba)
        matches_single = [orig for norm, orig in vocab_dict.items() if norm.startswith(norm_prefix)]
        
        # Û². Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú©Ù„Ù…Ø§Øª Ø¯ÙˆÚ¯Ø§Ù†Ù‡ Ú†Ø³Ø¨ÛŒØ¯Ù‡ (Ù…Ø«Ù„ iyimisin Ú©Ù‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒØ¯Ù‡ iyi misin)
        matches_concat = [combo for concat_norm, combo in concat_dict.items() if concat_norm.startswith(norm_prefix)]
        
        # ØªØ±Ú©ÛŒØ¨ Ù‡Ø± Ø¯Ùˆ Ù„ÛŒØ³Øª Ùˆ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        all_matches = list(dict.fromkeys(matches_single + matches_concat))
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ùˆ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Û³ ØªØ§ÛŒ Ø§ÙˆÙ„
        suggestions = sorted(all_matches, key=len)[:3]
        
    elif previous_word:
        norm_prev = normalize(previous_word)
        if norm_prev in bigram_model:
            top_next_words = bigram_model[norm_prev].most_common(3)
            suggestions = [word for word, count in top_next_words]
        
    return {"suggestions": suggestions}